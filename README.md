# CNN Multiclass Image Classification with PyTorch ‚Äî 3‚ÄëPart Project (Step‚Äëby‚ÄëStep)

This repo contains a 3‚Äëstage workflow for building and improving a **Convolutional Neural Network (CNN)** image classifier using **PyTorch**. Follow **Steps 1 ‚Üí 2 ‚Üí 3** to reproduce results and iterate on model quality.

> **Dataset link:**
> **`(https://www.kaggle.com/datasets/ryanbadai/clothes-dataset)`**

---

## üìÅ Repository Layout
```
.
‚îú‚îÄ‚îÄ Part1_DeepLearning_CNN.ipynb     # Step 1: Baseline CNN
‚îú‚îÄ‚îÄ Part2_DeepLearning_CNN.ipynb     # Step 2: Data imports & preprocessing
‚îú‚îÄ‚îÄ Part3_DeepLearning_CNN.ipynb     # Step 3: Optimization & training strategy
‚îî‚îÄ‚îÄ README.md                         # This combined guide
```

---

## üß∞ Environment & Setup

**Dependencies:** `torch`, `torchvision`, `pillow`, `numpy`, `pandas`, `scikit-learn`, `matplotlib`

Install:
```bash
pip install -U torch torchvision torchaudio pillow numpy pandas scikit-learn matplotlib
```

(Optional) Set seeds for reproducibility in each notebook (Python/NumPy/PyTorch).

**Expected data layout (typical):**
```
data/
‚îú‚îÄ‚îÄ train/
‚îÇ   ‚îú‚îÄ‚îÄ class_0/  ‚îú‚îÄ‚îÄ class_1/  ‚îú‚îÄ‚îÄ ... ‚îú‚îÄ‚îÄ class_N/
‚îú‚îÄ‚îÄ val/
‚îÇ   ‚îú‚îÄ‚îÄ class_0/  ‚îú‚îÄ‚îÄ class_1/  ‚îú‚îÄ‚îÄ ... ‚îú‚îÄ‚îÄ class_N/
‚îî‚îÄ‚îÄ test/
    ‚îú‚îÄ‚îÄ class_0/  ‚îú‚îÄ‚îÄ class_1/  ‚îú‚îÄ‚îÄ ... ‚îú‚îÄ‚îÄ class_N/
```
> If your notebooks use different path variables (e.g., `data_dir`, `train_dir`, `valid_dir`, `test_dir`), update those first.

**Common transforms (from the notebooks):**
- `Resize`, `ToTensor` (+ `Normalize` if configured)
- Light augmentation: `RandomHorizontalFlip`, `RandomRotation`

---

## ‚úÖ Step 1 ‚Äî Baseline CNN (Part 1)

**Notebook:** `Part1_DeepLearning_CNN.ipynb`  
Goal: Build a working baseline with a custom **`CNN_Model`** and confirm the training loop, dataloaders, and evaluation are correct.

**What happens here**
- Load data and apply basic transforms.
- Define **`CNN_Model`** (custom `nn.Module` with `Conv2d`, `ReLU`, `MaxPool2d`, optional `BatchNorm`/`Dropout`, and a linear head).
- Train for a few epochs; print training/validation metrics.
- Sanity‚Äëcheck overfitting/underfitting and save notes for improvements.

**Run:**
```bash
jupyter notebook "Part1_DeepLearning_CNN.ipynb"
```

---

## ‚öôÔ∏è Step 2 ‚Äî Data Imports & Pre‚ÄëProcessing (Part 2)

**Notebook:** `Part2_DeepLearning_CNN.ipynb`  
Goal: Stabilize the **data pipeline** and reuse the best‚Äëperforming baseline. Keep transforms consistent and verify splits.

**What happens here**
- Re‚Äëload data with the same (or slightly refined) transforms.
- Ensure consistent **train/val/test** splits and class mapping.
- Reuse or slightly tweak **`CNN_Model`** from Step 1.
- Confirm metrics reproducibility; record best baseline as a reference.

**Run:**
```bash
jupyter notebook "Part2_DeepLearning_CNN.ipynb"
```

---

## üöÄ Step 3 ‚Äî Optimization & Training Strategy (Part 3)

**Notebook:** `Part3_DeepLearning_CNN.ipynb`  
Goal: Improve performance/robustness with **training strategy** changes.

**Potential changes explored**
- Optimizer/lr updates (`SGD`/`Adam`, tune **lr**).
- Regularization: `Dropout`, **weight decay**.
- (Optional) LR schedules: `StepLR`, `ReduceLROnPlateau`, `CosineAnnealingLR`, `OneCycleLR`.
- Early stopping/checkpointing if included.
- Track best epoch and compare to Step 1/2.

**Run:**
```bash
jupyter notebook "Part3_DeepLearning_CNN.ipynb"
```

---

## üß™ Metrics & Reporting

Each notebook prints metrics to the console (accuracy, and scaffolding for precision/recall/F1).  


**Tips**
- Save a **confusion matrix** and `classification_report` for per‚Äëclass insight.
- Log loss/accuracy curves per epoch and keep them in an `images/` folder.

---

## üîß Implementation Notes

- **Custom model:** notebooks define a `CNN_Model` (custom `nn.Module`); variants appear across steps.
- **Transforms observed:** `Resize`, `ToTensor`, with optional `Normalize`; light augments: `RandomHorizontalFlip`, `RandomRotation`.
- **Device:** CUDA if available, otherwise CPU.
- **Loss/Optimizers:** typical setup is `CrossEntropyLoss` with `SGD` or `Adam`.

---

## üîó Dataset Link (if provided)
https://www.kaggle.com/datasets/ryanbadai/clothes-dataset

---

## üß≠ Next Steps

- Add richer augmentation (`ColorJitter`, `RandomResizedCrop`), then re‚Äëbenchmark.
- Try **weight decay** and **LR schedules**; compare validation curves.
- Consider **transfer learning** (e.g., ResNet18/MobileNetV2) as a strong baseline.
- Export best checkpoints to `checkpoints/` and embed plots in `images/` within this README.
